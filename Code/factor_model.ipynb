{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "\n",
    "\n",
    "class FactorModel(nn.Module):\n",
    "    def __init__(self, N: int, K: List[int], d: int):\n",
    "        super(FactorModel, self).__init__()\n",
    "        self.N = N  # Number of questions\n",
    "        self.K = K  # List of number of categorical answers for each question\n",
    "        self.d = d  # Latent dimension\n",
    "        self.W = nn.ModuleList([nn.Linear(d, K[i] - 1, bias=False) for i in range(N)])\n",
    "        self.biases = nn.ParameterList(\n",
    "            [nn.Parameter(torch.randn(K[i] - 1)) for i in range(N)]\n",
    "        )\n",
    "\n",
    "    def forward(self, v: torch.Tensor) -> List[torch.Tensor]:\n",
    "        logits = [self.W[n](v) + self.biases[n] for n in range(self.N)]\n",
    "        # Append zero logits for the last category to each question's logits\n",
    "        logits = [\n",
    "            torch.cat((logit, torch.zeros(logit.size(0), 1).to(logit.device)), dim=1)\n",
    "            for logit in logits\n",
    "        ]\n",
    "        return logits\n",
    "\n",
    "    def predict_proba(self, v: torch.Tensor) -> List[torch.Tensor]:\n",
    "        logits = self.forward(v)  # List of logits for each question\n",
    "        probabilities = [torch.softmax(logit, dim=-1) for logit in logits]\n",
    "        return probabilities\n",
    "\n",
    "\n",
    "def loss_function(\n",
    "    model: FactorModel,\n",
    "    v: torch.Tensor,\n",
    "    answers: torch.Tensor,\n",
    "    lambda1: float,\n",
    "    lambda2: float,\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    criterion = nn.NLLLoss(reduction=\"sum\")\n",
    "    logits = model.forward(v)\n",
    "\n",
    "    total_loss = 0\n",
    "    for n in range(model.N):\n",
    "        log_probs = nn.functional.log_softmax(logits[n], dim=1)\n",
    "        total_loss += criterion(log_probs, answers[:, n])\n",
    "    nll_loss = total_loss.clone()\n",
    "    # L2 Regularization\n",
    "    l2_reg = sum(torch.norm(W.weight, 2) ** 2 for W in model.W)\n",
    "    total_loss += lambda1 * l2_reg\n",
    "    total_loss += lambda2 * torch.norm(v, 2) ** 2\n",
    "    return total_loss, nll_loss\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: FactorModel,\n",
    "    answers: torch.Tensor,\n",
    "    epochs: int = 1000,\n",
    "    lr: float = 0.01,\n",
    "    lambda1: float = 0.01,\n",
    "    lambda2: float = 0.01,\n",
    ") -> Tuple[FactorModel, torch.Tensor]:\n",
    "\n",
    "    v = torch.randn(len(answers), model.d, requires_grad=True)\n",
    "\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer = optim.Adam(list(model.parameters()) + [v], lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss, nllloss = loss_function(model, v, answers, lambda1, lambda2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # if epoch % 100 == 0:\n",
    "        # print(f\"Epoch {epoch}, Loss: {loss.item()}, nll_loss: {nllloss.item()}\")\n",
    "    print(\n",
    "        f\"d: {model.d}, Epoch {epoch}, Loss: {loss.item()}, nll_loss: {nllloss.item()}\"\n",
    "    )\n",
    "    return model, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/processed/narrative_questions.json\") as f:\n",
    "    data = json.load(f)\n",
    "Q = data.values()\n",
    "K = []\n",
    "for q in Q:\n",
    "    choice = q[\"choices\"].values()\n",
    "    K.append(len(choice))\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def extract_answers(file):\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Initialize a list to store the extracted values\n",
    "    answers = []\n",
    "\n",
    "    # Iterate through the JSON objects to extract `post_LLM_answer`\n",
    "    for item in data:\n",
    "\n",
    "        if \"Narrative_answers\" in item:\n",
    "            # get the value under the key `answers`\n",
    "            answer = item[\"Narrative_answers\"]\n",
    "            answers.append(list(answer.values()))\n",
    "\n",
    "    # convert the list of extracted values to a numpy array\n",
    "    answers = np.array(answers, dtype=object)\n",
    "    answers = np.where(answers == \"A\", 0, answers)\n",
    "    answers = np.where(answers == \"B\", 1, answers)\n",
    "    answers = np.where(answers == \"C\", 2, answers)\n",
    "    print(answers)\n",
    "    answers = answers.astype(int)\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 2 2 2 2]\n",
      " [2 2 2 2 2 2]\n",
      " [2 2 2 2 2 2]\n",
      " ...\n",
      " [2 2 2 2 2 2]\n",
      " [2 1 2 2 2 2]\n",
      " [0 2 2 2 2 2]]\n",
      "(1138, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers = []\n",
    "#for file in file_list:\n",
    "answer = extract_answers(\"../data/processed/UN_data_with_narrative_answers.json\")\n",
    "print(answer.shape)\n",
    "answers.append(answer)\n",
    "answers = np.array(answers)\n",
    "\n",
    "# reshape answers to 600,80\n",
    "answers = answers.reshape(1138, 6)\n",
    "answers.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_max = list(answers.max(axis=0) + 1)\n",
    "for i in range(len(ans_max)):\n",
    "    if ans_max[i] > K[i]:\n",
    "        print(i + 1, ans_max[i], K[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = torch.tensor(answers, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(K)  # Number of questions\n",
    "\n",
    "M = answers.shape[0]  # Number of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 3, Epoch 19999, Loss: 382.9897155761719, nll_loss: 302.0203552246094\n"
     ]
    }
   ],
   "source": [
    "epo = 20000\n",
    "\n",
    "for d in [3]:\n",
    "    model = FactorModel(N, K, d)\n",
    "    trained_model, v = train_model(\n",
    "        model, answers, epochs=epo, lr=0.01, lambda1=0.01, lambda2=0.01\n",
    "    )\n",
    "    # Extract the latent vectors v\n",
    "    latent_vectors = v.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your directory and filename\n",
    "directory = \"../data/processed/\"  # Replace with your desired directory path\n",
    "filename = f\"latent_vector_{d}.csv\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Construct the full file path\n",
    "file_path = os.path.join(directory, filename)\n",
    "\n",
    "# Save the latent vectors to the CSV file at the specified path\n",
    "np.savetxt(\n",
    "    file_path,\n",
    "    latent_vectors,\n",
    "    delimiter=\",\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
